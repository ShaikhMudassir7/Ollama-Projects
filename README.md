
# Ollama-Projects
This repo will cover how we can set up and use Ollama, pooling and customizing models, Rest APIs, Python Integrations, Groceries Organizer, Rag System and AI Recruitment Agency
![image](https://github.com/user-attachments/assets/a5d1789e-ac1a-4ce3-9b9c-2b50069f6f61)

WE CAN USE MODELS LIKE:
1. Llama 3
2. Mistral
3. Phi 3
4. Gemma 2
5. And many more
   
## What does this Ollama Repo Consist of?
1. Ollama
   - Building local LLM Applications using Ollama:
     1. Customize Models
     2. Using different Ollama Models (Testing..)
     3. Building RAG System using Ollama Models
     4. Tools/Function Calling
     5. Build full-fledged LLM Applications using Ollama Models

## Who is this course for?
1. Machine Learning Engineers
2. AI Engineers
3. Developers
4. Data Scientists

## Prerequisites
1. Know programming (python preferably)
2. Basics of AI, LLMS, ML

## Structure of the Repo 
1. Theory
2. Hands-On
3. Mixed of Both

## Things we need 
1. Python
2. VSCode 

# Ollama Dive Deep

## What is Ollama
- Ollama is an open-source tool that is designed to develop simplified LLM models locally (i.e Llama 1,2,3,4 and 5)
- Also it is free to use, it allows us to pick different large language model
- At its core Ollama uses CLI (Command Line Interface) which helps us install and executions LLMs locally
- it reduces the technicality to setup the LLM which makes it easier to set up locally
- It profoundly helps in the generation of the RAG Systems ( Documents -> Chunks(chunk1, chunk2, chunk3,...) -> Embedding LLM (Helps in representing the vector representation of these chunks) -> Embedding (Emb1, Emb2, Emb3,... ) -> Vector Similarities and Responses -> Gen LLM (with query embeddings) -> Response)
  ![image](https://github.com/user-attachments/assets/e46e9ff0-363c-44de-b24b-6058c85654c4)

## Advantages of Ollama
1. Free usage of LLMs
2. Increased Customizability
3. Privacy Concerns - Running LLMs locally brings a lot of security
4. Ease of use - Setting up LLMs becomes easy
5. Cost Efficiency - as there's no Cloud Based usage
6. Latency Reduction - local execution reduces the latency issues i.e less delays (which basically means the reduction of the time of hitting the server)
7. Customization - flexibility to customize our model

## Key Features of Ollama

1.  Model Download and Management
2.  Unified Interface - we can interact with various models using one single Interface
3.  Extensibility - the power to customise the model based on our requirement
4.  Performance Optimization - we'll be able to utilize our own GPU and CPU according to the required need


## Use Cases of Ollama
1. Development and Testing
2. Education and Research
3. Secured Applications

## Setting up Ollama Locally
1. Installation
   - Go to https://ollama.com
   - Click on the Download button
   ![image](https://github.com/user-attachments/assets/a73e2111-e87d-4829-99b2-50ea6613cfdc)
   - 
3. Setup

## System Requirements
- OS:
     - Windows
     - Mac
     - Linux
- Storage ~ 10GB
- Processor: Modern CPU

